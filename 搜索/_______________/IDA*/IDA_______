IDA*算法是A*算法和迭代加深算法的结合。

 

迭代加深算法是在dfs搜索算法的基础上逐步加深搜索的深度，它避免了广度优先搜索占用搜索空间太大的缺点，也减少了深度优先搜索的盲目性。它主要是在递归搜索函数的开头判断当前搜索的深度是否大于预定义的最大搜索深度，如果大于，就退出这一层的搜索，如果不大于，就继续进行搜索。这样最终获得的解必然是最优解。

 

而在A*算法中，我们通过使用合理的估价函数，然后在获得的子节点中选择fCost最小的节点进行扩展，以此完成搜索最优解的目的。但是A*算法中，需要维护关闭列表和开放列表，需要对扩展出来的节点进行检测，忽略已经进入到关闭列表中的节点（也就是所谓的“已经检测过的节点”），另外也要检测是否与待扩展的节点重复，如果重复进行相应的更新操作。所以A*算法主要的代价花在了状态检测和选择代价最小节点的排序上，这个过程中占用的内存是比较大的，一般为了提高效率都是使用hash进行重复状态检测。

 

而IDA*算法具有如下的特点：（

综合了A*算法的人工智能性和回溯法对空间的消耗较少的优点，在一些规模很大的搜索问题中会起意想不到的效果。它的具体名称是 Iterative Deepening A*, 1985年由Korf提出。该算法的最初目的是为了利用深度搜索的优势解决广度A*的空间问题，其代价是会产生重复搜索。归纳一下，IDA*的基本思路是：首先将初始状态结点的H值设为阈值maxH，然后进行深度优先搜索，搜索过程中忽略所有H值大于maxH的结点；如果没有找到解，则加大阈值maxH，再重复上述搜索，直到找到一个解。在保证H值的计算满足A*算法的要求下，可以证明找到的这个解一定是最优解。在程序实现上，IDA* 要比 A* 方便，因为不需要保存结点，不需要判重复，也不需要根据 H值对结点排序，占用空间小。
而这里在IDA*算法中也使用合适的估价函数，来评估与目标状态的距离。

在一般的问题中是这样使用IDA*算法的，当前局面的估价函数值+当前的搜索深度 > 预定义的最大搜索深度时，就进行剪枝。

这个估计函数的选取没有统一的标准，找到合适的该函数并不容易，但是可以大致按照这个原则：在一定范围内加大各个状态启发函数值的差别。

